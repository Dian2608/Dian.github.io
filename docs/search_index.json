[["index.html", "Portfolio Chapter 1 Introduction", " Portfolio Dian Dupon 2023-08-23 Chapter 1 Introduction My name is Dian and I am a third-year Life Sciences student at the Hogeschool Utrecht. In this third year I specialized in microbiology and data science. This portfolio was created on behalf of the Data Science for Biology 2 course. "],["c.-elegans-offspring-count-with-different-components.html", "Chapter 2 C. elegans offspring count with different components 2.1 Installing packages 2.2 Importing the data 2.3 Scatterplot of the C. elegans plate experiment 2.4 Conclusion 2.5 Further analysis", " Chapter 2 C. elegans offspring count with different components 2.1 Installing packages # install.packages(&quot;gitcreds&quot;) # library(gitcreds) # gitcreds_set() 2.2 Importing the data For this experiment the data from the CE.LIQ.FLOW.062_Tidydata.xlsx file was used to determine the effect of compound concentrations on offspring count and whether the different compounds have a positive effect on the amount of offspring. After importing the data, the data types of columns were checked to see if these were correctly assigned. # Loading library library(tidyverse) library(readxl) # Getting the data from the downloaded Excel file # You can find the raw data in the Data_raw folder of this experiment elegans_data &lt;- read_excel(&quot;./Data_raw/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) # Check if the data imported correctly head(elegans_data) ## # A tibble: 6 × 34 ## plateRow plateColumn vialNr dropCode expType expReplicate expName ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NA NA 1 a experiment 3 CE.LIQ.FLOW.062 ## 2 NA NA 1 b experiment 3 CE.LIQ.FLOW.062 ## 3 NA NA 1 c experiment 3 CE.LIQ.FLOW.062 ## 4 NA NA 1 d experiment 3 CE.LIQ.FLOW.062 ## 5 NA NA 1 e experiment 3 CE.LIQ.FLOW.062 ## 6 NA NA 2 a experiment 3 CE.LIQ.FLOW.062 ## # ℹ 27 more variables: expDate &lt;dttm&gt;, expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;, ## # expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;dbl&gt;, compCASRN &lt;chr&gt;, ## # compName &lt;chr&gt;, compConcentration &lt;chr&gt;, compUnit &lt;chr&gt;, ## # compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;, ## # elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, ## # bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, ## # bacterialVolUnit &lt;chr&gt;, incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, … # Check data types of specific columns str(elegans_data$RawData) ## num [1:360] 44 37 45 47 41 35 41 36 40 38 ... str(elegans_data$compName) ## chr [1:360] &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; ... str(elegans_data$compConcentration) ## chr [1:360] &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; ... The data type of the column compConcentration has not been correctly assigned during the importing of the data into R. Therefore we need to change the data type of this column to numeric. # Change column data type to numeric elegans_data_tidy &lt;- transform(elegans_data, compConcentration = as.numeric(compConcentration)) # Check if it is correctly assigned str(elegans_data_tidy$compConcentration) ## num [1:360] 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 ... After making the data from the excel file tidy a scatterplot is created to study the data of the plate experiment more thoroughly. Geom_jitter is used so that the data points don’t overlap. The positive control for this experiment is ethanol. The negative control for this experiment is S-medium. 2.3 Scatterplot of the C. elegans plate experiment # Load library library(ggplot2) # Creating the scatterplot elegans_data_tidy %&gt;% ggplot(aes(x=log10(compConcentration+0.00005), y= RawData))+ #Adding 0.0005 to prevent data loss geom_jitter(aes(colour=compName, shape = expType), width = 0.05)+ theme_bw()+ labs(title = &quot;C. elegans offspring count&quot;, x = &quot;Compound Concentration (nM)&quot;, y = &quot;Rawdata (offspring counts)&quot;) 2.3.1 Normalizing the data You want to normalize the data to ensure that any differences we observe in the data between the different compounds and concentrations are not simply due to differences in the overall baseline level of the response. In the figure below, the data is normalized to the negative control (S-medium). The offspring count from C. elegans incubated in S-medium is equal to 1.0. # Calculate the normalization factor norm_factor &lt;- mean(elegans_data_tidy$RawData[elegans_data_tidy$expType == &quot;controlNegative&quot;]) # Normalize the data elegans_data_tidy$Normalized &lt;- elegans_data_tidy$RawData / norm_factor # Setting the negative control value to 1 elegans_data_tidy$Normalized[elegans_data_tidy$expType == &quot;Negative Control&quot;] &lt;- 1 # Creating the plot with the normalized data elegans_data_tidy %&gt;% ggplot(aes(x = log10(compConcentration+0.00005), y = Normalized)) + geom_jitter(aes(colour= compName, shape= expType), width = 0.05)+ labs(title = &quot;Normalized C. elegans offspring count&quot;, x = &quot;Compound Concentration (nM)&quot;, y = &quot;Normalized Rawdata (offspring counts&quot;) 2.4 Conclusion Based on the graphs above there can be concluded that 2.6-diisopropylnaphtalene, decane and nepthalene all cause a decrease in the amount of C. elegans offspring. 2.5 Further analysis To analyse if there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve it is advisable to make a dosis-response curve (IC50). To make this curve you can to follow the next few steps: 1. Normalize the data to the controlNegative condition. 2. Fit a dose-response curve to the data for each compound using a four-parameter logistic model. 3. Estimate the IC50 value for each compound based on the fitted curve. 4. Compare the IC50 values across the different compounds to see if there are any differences in growth of the C. elegans offspring. 5. Perform statistical tests to determine whether there is a significant effect of concentration on offspring count for each compound. "],["scoring-a-publication-on-reproducibility.html", "Chapter 3 Scoring a publication on reproducibility 3.1 Information about the study 3.2 Reproducibility scoring 3.3 Running the open source code", " Chapter 3 Scoring a publication on reproducibility Often times the used R code for rendering and inspecting data is not visualized in a publication. In this file the article “Meta-Analysis: MRI Volumetric Data of Children with ADHD Subtypes” is tested and scored on reproducibility. The article can be found under the following link: https://osf.io/d97pw/ 3.1 Information about the study The aim of this study was to examine how the ADHD subtypes differentiate based on brain structure volume size. Attention-deficit hyperactivity disorder (ADHD) is a common neurodevelopmental disorder consisting of inattentive and/or hyperactive behaviors that is typically prevalent in childhood. There are three recognized subtypes of this disorder—hyperactive, inattentive, and combined. For this study a meta-analysis was done using 8 studies that included volumetric data of ADHD subtypes (inattentive and combined) in children that was acquired through magnetic resonance imaging (MRI) techniques. Analyses were done looking at combined and inattentive type in comparison to controls and between the two groups. Further subgroup analyses were done on gender and brain regions in the two subtypes. Results show that there is a significant brain volume reduction in combined type in comparison to controls and inattentive type. There is also a significant volume reduction observed in males. The other analyses done yielded insignificant findings, although the volume reduction in inattentive type was only slightly above the cutoff of alpha (0.05). These findings help in better understanding the relations between brain volume and ADHD subtypes, but further research is still needed in this area. 3.2 Reproducibility scoring The article is going to be scored on the basis of ‘Repita’ criteria. The criterea will be scored on a scale from 1 (very hard) to 5 (very easy). More information about these criteria can be found under the following link: https://www.researchgate.net/publication/340244621_Reproducibility_and_reporting_practices_in_COVID-19_preprint_manuscripts Transparency Criteria Score on a 1-5 scale Study Purpose 4 Data Availability Statement 4 Data Location 5 Study Location 3 Author Review 4 Ethics Statement 3 Funding Statement 1 Code Availability 5 3.3 Running the open source code The file “HYSELL_Meta_Studies.xlsx” was downloaded of the site to run the code and see if it is reproducible. In terms of the readability of the code i would grade the code a score of 4 out of 5. When running the script i have not encountered major difficulties with visualizing a figure. The only thing i had to change from the original script is the function “plyr::revalue”. Rstudio did not reconize this function. With this in mind i have changed this piece of the code to “dplyr::recode”. Taken together on a scale from 1 (very hard) to 5 (very easy) it took much effort to reproduce the visualization of the data. With this in mind i would score the article a 4 out of 5. library(dplyr) library(metafor) library(tidyverse) library(robumeta) library(readxl) HYSELL_Meta_Studies &lt;- read_excel(&quot;./Data_raw/HYSELL_Meta_Studies.xlsx&quot;) # Convert ABB column to factor HYSELL_Meta_Studies$stype &lt;- as.factor(HYSELL_Meta_Studies$ABB) # Mutate stype column using dplyr::recode HYSELL_Meta_Studies &lt;- HYSELL_Meta_Studies %&gt;% mutate(stype = dplyr::recode(ABB, &quot;C&quot; = &quot;0&quot;, &quot;IA&quot; = &quot;1&quot;)) ## tn: Treatment Group Sample size ## cn: control Group Sample size ## tmean: Treatment Group Mean ## cmean: Control Group Mean ## tsd: Treatment Group Standard Deviation ## csd: Control Group Standard Deviation ##Effect Size Calculation #Cohen&#39;s d #Common Components HYSELL_Meta_Studies$IG_totaln &lt;- with(HYSELL_Meta_Studies, tn+cn) HYSELL_Meta_Studies$IG_multin &lt;- with(HYSELL_Meta_Studies, tn*cn) #Step 1. Pooled Standard Deviation HYSELL_Meta_Studies$s_pool &lt;- with(HYSELL_Meta_Studies, sqrt(((tn-1)*(tsd^2)+(cn-1)*(csd^2))/(IG_totaln-2))) #Step 2. Effect Size HYSELL_Meta_Studies$IG_d &lt;- with(HYSELL_Meta_Studies, (tmean-cmean)/s_pool) #Step 3. Sampling Variance HYSELL_Meta_Studies$IG_se &lt;- with(HYSELL_Meta_Studies, (IG_totaln/IG_multin)+(IG_d^2/(2*IG_totaln))) ## MERGE Cohen&#39;s d Effect Size into one column HYSELL_Meta_Studies$Cohen_es &lt;-rowSums(select(HYSELL_Meta_Studies, ends_with(&quot;_d&quot;)), na.rm=T) HYSELL_Meta_Studies$Cohen_v &lt;-rowSums(select(HYSELL_Meta_Studies, ends_with(&quot;_se&quot;)), na.rm=T) #column for effect size and sampling variance #Based on Cohen&#39;s d, HYSELL_Meta_Studies$Cohen_es &lt;- as.numeric(HYSELL_Meta_Studies$Cohen_es, na.rm=TRUE) HYSELL_Meta_Studies$Cohen_v &lt;- as.numeric(HYSELL_Meta_Studies$Cohen_v, na.rm=TRUE) ##Bias-Corrected Standardized Mean Difference (Hedges&#39; g) from Cohen&#39;s d #Correction Factor (J) #Reclassified effect size IG ( =1) HYSELL_Meta_Studies$df &lt;- HYSELL_Meta_Studies$IG_totaln-2 HYSELL_Meta_Studies$g_j &lt;- 1-(3/((4*HYSELL_Meta_Studies$df)-1)) HYSELL_Meta_Studies$g_es_all &lt;- with(HYSELL_Meta_Studies, Cohen_es*g_j) HYSELL_Meta_Studies$g_v_all &lt;- with(HYSELL_Meta_Studies, Cohen_v*g_j^2) ## Random Effect thesis_ran_model &lt;- rma(g_es_all, g_v_all, data = HYSELL_Meta_Studies) thesis_ran_model ## ## Random-Effects Model (k = 95; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.0492 (SE = 0.0133) ## tau (square root of estimated tau^2 value): 0.2218 ## I^2 (total heterogeneity / total variability): 66.31% ## H^2 (total variability / sampling variability): 2.97 ## ## Test for Heterogeneity: ## Q(df = 94) = 239.2705, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## -0.1799 0.0325 -5.5306 &lt;.0001 -0.2437 -0.1162 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Number of effect sizes: 95 #Pooled Effect size: -0.1799 #Standard error of pooled effect size: 0.0325 #95% Confidence interval: [-0.2437, -0.1162] #p-value: &lt;.0001 # Q-statistic = 239.2705 # df = 94 # p-val = &lt; 0.0001 #Interpretation: # Moderator Analysis: Comparing inattentive with combined: a Q-test based on analysis of variance moder1_t &lt;- rma(g_es_all, g_v_all, data=HYSELL_Meta_Studies, subset=stype== 0) # For combined group (=0) moder2_t &lt;- rma(g_es_all, g_v_all, data=HYSELL_Meta_Studies, subset=stype== 1) # For inattentive group (=1) moder1_t ## ## Random-Effects Model (k = 52; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.0965 (SE = 0.0296) ## tau (square root of estimated tau^2 value): 0.3106 ## I^2 (total heterogeneity / total variability): 79.80% ## H^2 (total variability / sampling variability): 4.95 ## ## Test for Heterogeneity: ## Q(df = 51) = 143.6559, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## -0.2564 0.0547 -4.6830 &lt;.0001 -0.3637 -0.1491 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 moder2_t ## ## Random-Effects Model (k = 43; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.0078 (SE = 0.0069) ## tau (square root of estimated tau^2 value): 0.0881 ## I^2 (total heterogeneity / total variability): 22.52% ## H^2 (total variability / sampling variability): 1.29 ## ## Test for Heterogeneity: ## Q(df = 42) = 60.5712, p-val = 0.0316 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## -0.0579 0.0309 -1.8701 0.0615 -0.1185 0.0028 . ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Pooled Effect size combined type: -0.2564 (SE = 0.0547, p &lt;.0001) # Within-variance, Q(df = 51) = 143.6559, p-val &lt;.0001 # Pooled Effect size for inattentive type: -0.0579 (SE = 0.0309, p= 0.0615) # Within-variance, Q(df = 42) = 60.5712, p-val = 0.0316 # Overall pooled Effect size: -0.1799 (SE = 0.0325, p &lt;.0001) # Total Variance Q(df = 94) = 239.2705,, p-val &lt; .0001 143.6559 + 60.5712 ## [1] 204.2271 # Qwithin = 204.2271 239.2705 - (143.6559 + 60.5712) ## [1] 35.0434 # Qbetween = 35.0434 # Test under chi-square distribution (Q-statistic follows chi-square dsictribution.) pchisq(35.0434, df=1, lower.tail=FALSE) ## [1] 3.22438e-09 # p-value for the Qbetween is 3.22438e-09 (less than alpha-level, 0.05). # We found that the Q-between (35.0434) is significant (p &lt; 0.05) indicating # two groups have statistically different pooled effect sizes. ## Meta-regression without and with &quot;% of male&quot; moderator # Centering the moderator mean(HYSELL_Meta_Studies$m2f) ## [1] 0.7951579 HYSELL_Meta_Studies$m2f_c &lt;- HYSELL_Meta_Studies$m2f - 0.795 ## RVE (Considering dependency among multiple effect sizes within a study) RVE_thesis &lt;- robu(formula = g_es_all ~ 1, data=HYSELL_Meta_Studies, studynum=studyid, var.eff.size=g_v_all, modelweights = &quot;CORR&quot;, small = TRUE) RVE_thesis ## RVE: Correlated Effects Model with Small-Sample Corrections ## ## Model: g_es_all ~ 1 ## ## Number of studies = 8 ## Number of outcomes = 95 (min = 4 , mean = 11.9 , median = 8 , max = 24 ) ## Rho = 0.8 ## I.sq = 64.62675 ## Tau.sq = 0.09790346 ## ## Estimate StdErr t-value dfs P(|t|&gt;) 95% CI.L 95% CI.U Sig ## 1 X.Intercept. -0.163 0.0701 -2.33 6.45 0.0556 -0.332 0.00523 * ## --- ## Signif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 * ## --- ## Note: If df &lt; 4, do not trust the results #Number of effect sizes: 95 #Number of studies: 8 #Pooled Effect size: -0.163 #Standard error of pooled effect size: 0.0701 #95% Confidence interval: [-0.332, 0.00523] #p-value: 0.0556 # Tau-square: 0.09790346 #RVE with moderator RVE_thesis1 &lt;- robu(formula = g_es_all ~ m2f_c, data=HYSELL_Meta_Studies, studynum=studyid, var.eff.size=g_v_all, modelweights = &quot;CORR&quot;, small = TRUE) RVE_thesis1 ## RVE: Correlated Effects Model with Small-Sample Corrections ## ## Model: g_es_all ~ m2f_c ## ## Number of studies = 8 ## Number of outcomes = 95 (min = 4 , mean = 11.9 , median = 8 , max = 24 ) ## Rho = 0.8 ## I.sq = 62.06272 ## Tau.sq = 0.08758224 ## ## Estimate StdErr t-value dfs P(|t|&gt;) 95% CI.L 95% CI.U Sig ## 1 X.Intercept. -0.148 0.0468 -3.16 5.53 0.0219 -0.264 -0.0308 ** ## 2 m2f_c -1.105 0.2403 -4.60 2.74 0.0235 -1.913 -0.2971 ** ## --- ## Signif. codes: &lt; .01 *** &lt; .05 ** &lt; .10 * ## --- ## Note: If df &lt; 4, do not trust the results #Number of effect sizes: 95 #Number of studies: 8 #Pooled Effect size: -0.148 #Standard error of pooled effect size: 0.0468 #95% Confidence interval: [-0.264, -0.0308] #p-value: 0.0219 # Tau-square: 0.08758224 #Pooled Effect size for gender: -1.105 #Standard error of pooled effect size for gender: 0.2403 #95% Confidence interval for gender: [-1.913, -0.2971] #p-value gender: 0.0235 ## Make Forest plot forest(thesis_ran_model, slab = paste(HYSELL_Meta_Studies$Authors), xlim = c(-2,2), cex = 0.5) op &lt;- par(cex = 0.75, font = 10) text(-3, 98, &quot;Author and Year&quot;, pos = 4) text(2, 98, &quot;Hedges&#39;g [95% CI]&quot;, pos = 2) par(op) "],["guerrilla-analytics-structure.html", "Chapter 4 Guerrilla analytics structure", " Chapter 4 Guerrilla analytics structure The folder tree below shows my folder structure of the DAUR2 lessons. These lessons were are a part of the data science for biology 1 course at Hogeschool Utrecht. Folder structure DAUR2 course "],["curriculum-vitae.html", "Chapter 5 Curriculum vitae", " Chapter 5 Curriculum vitae The figure below shows my Curriculum vitae (CV). Some information is missing from the CV. This is because of privacy reasons. Curriculum vitae "],["analyzing-mass-spectrometry-data-in-r.html", "Chapter 6 Analyzing mass spectrometry data in R 6.1 Introduction 6.2 Planning 6.3 Data import 6.4 Spectra 6.5 Annotation for each spectrum 6.6 Visualization of the data 6.7 Analyzing MALDI-TOF MS data 6.8 Conclusion", " Chapter 6 Analyzing mass spectrometry data in R 6.1 Introduction Since my last study I have become very interested in microbiology. This has led me to choose to follow my specialization in microbiology at my current study Life Sciences. This has made me even more enthusiastic about the profession. In 2 years time I see myself working in diagnostics at a medical microbiology laboratory. To achieve this goal, I will complete my graduation internship in a similar direction as well. In microbiology, MALDI-TOF MS is currently a widely used method to quickly identify a bacterial species. For this reason, I thought it would be interesting to process raw data from a mass spectrometry experiment in R and visualize the results. MALDI-TOF MS, or Matrix Assisted Laser Desorption/Ionization Time of Flight Mass Spectrometry, separates ions by their mass to charge ratio and determines that mass to charge ratio by the time it takes for the ions to reach a detector. 6.2 Planning The first thing I’m going to do is delve into the analysis of mass spectrometry data in R. Have packages already been made for the analysis of MALDI-TOF MS data or mass spectrometry data in general? To achieve my goal I will take the following steps: - Finding articles about the subject of mass spectrometry - Finding mass spectrometry data (preferably from a MALDI-TOF experiment) - Finding out the structure of mass spectrometry data files - Writing code to visualize mass spectrometry data - Writing code to visualize or highlight specific m/z values in a figure 6.3 Data import To get started with the processing of raw mass spectrometry data I will first need to find some data to work with. There are several packages with mass spectrometry data available. I used the data from the msdata package. The msdata data package provides multiple raw data mass spectrometry files. # Load library library(msdata) # Get files from msdata fls &lt;- proteomics(full.names = TRUE) basename(fls) ## [1] &quot;MRM-standmix-5.mzML.gz&quot; ## [2] &quot;MS3TMT10_01022016_32917-33481.mzML.gz&quot; ## [3] &quot;MS3TMT11.mzML&quot; ## [4] &quot;TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz&quot; ## [5] &quot;TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz&quot; # Path variable to the file of choice ## I chose the second file, but you can change it to any you like. It is also possible to do all files. path &lt;- fls[2] # Check path ## [1] &quot;C:/Users/diand/AppData/Local/R/win-library/4.2/msdata/proteomics/MS3TMT10_01022016_32917-33481.mzML.gz&quot; Now that I have provided the file path, I will try to access the data with the mzR package. The mzR package aims at providing a common, low-level interface to several mass spectrometry data formats. With this package I can check the number of spectra, which is important for a identification for instance. # Load library library(mzR) # Get the MS file from the path variable msfile &lt;- openMSfile(path, backend = NULL, verbose = FALSE) # Check number of spectra msfile ## Mass Spectrometry file handle. ## Filename: MS3TMT10_01022016_32917-33481.mzML.gz ## Number of scans: 565 6.4 Spectra We can check the spectra of the file we just loaded into R with the spectra function of the mzR package. It is possible to see specific spectra of the file by providing the index of a spectrum, but it is also possible to just check all spectra. Because I am not looking for a specific spectra I will check the spectra of sample 1. I also demonstrated the usage of the spectra function if you would like to look at two specific spectra or all spectra. The output is a matrix which consists of two columns. One column for the mflz-values and the second column shows the intensity’s for the mflz-files. If you load all spectra, in this case 565 spectra, it gives a lot of output. Under the guise of keeping this Rmarkdown tidy, I won’t show the output of the code to load all spectra. # Read one spectra from the input file spectra1 &lt;- spectra(msfile, 1) head(spectra1) ## mz intensity ## [1,] 376.2217 0 ## [2,] 376.2227 0 ## [3,] 376.2237 0 ## [4,] 376.2247 0 ## [5,] 380.7361 0 ## [6,] 380.7371 0 # Read multiple specific spectra spectra2 &lt;- spectra(msfile, 1:2) head(spectra2) # Read all spectra from the input file ## all_spectra &lt;- spectra(msfile) ## head(all_spectra) # Length of the spectra length(spectra1) ## [1] 96608 6.4.1 Visualisation spectra To get a better overview of what exactly the spectra entail, I wanted to visualize the spectra. This can also be done with the mzR package. I used the peaks function to extract the peaks and visualize them in a plot. In this case spectrum 8 was used. # Read a spectrum from the file peak &lt;- peaks(msfile, 8) head(peak) ## mz intensity ## [1,] 182.7910 39.12186 ## [2,] 197.1867 46.60393 ## [3,] 198.4393 92.85271 ## [4,] 199.1400 52.84999 ## [5,] 201.2341 42.46858 ## [6,] 204.3054 54.46336 # Count of the spectra peaks peaksCount(msfile, 8) ## [1] 858 # Plot the peaks plot(peak[,1], peak[,2], type = &quot;h&quot;, lwd=1) 6.5 Annotation for each spectrum Now that I have verified the presence of the spectra in the data, we can look at the corresponding annotation. This annotation is necessary to distinguish the spectra of different samples. In the data file I used, the different samples are encoded with numbers. # Check annotation only hdr &lt;- header(msfile) head(hdr) ## seqNum acquisitionNum msLevel polarity peaksCount totIonCurrent retentionTime ## 1 1 32918 1 1 48304 3005937408.0 4422.620 ## 2 2 32919 2 1 755 1025988.7 4422.648 ## 3 3 32920 2 1 803 697426.6 4422.670 ## 4 4 32921 2 1 765 1220569.9 4422.735 ## 5 5 32922 2 1 499 494725.3 4422.800 ## 6 6 32923 3 1 2540 2522363.0 4423.036 ## basePeakMZ basePeakIntensity collisionEnergy ionisationEnergy lowMZ ## 1 696.7169 172620640.00 NA 0 376.2217 ## 2 646.4719 121457.00 35 0 187.1043 ## 3 641.8309 60390.23 35 0 188.2159 ## 4 549.6967 103048.61 35 0 200.2342 ## 5 718.6267 114702.83 35 0 166.0463 ## 6 129.1378 275641.03 65 0 99.0060 ## highMZ precursorScanNum precursorMZ precursorCharge precursorIntensity ## 1 1515.1263 NA NA NA NA ## 2 1645.9059 32918 652.3426 3 9841531 ## 3 1701.1073 32918 647.3441 3 3921567 ## 4 1596.3099 32918 673.3353 3 7623700 ## 5 1374.6786 32918 575.0026 3 2357085 ## 6 505.0462 NA 490.2629 0 0 ## mergedScan mergedResultScanNum mergedResultStartScanNum ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## mergedResultEndScanNum injectionTime ## 1 NA 0.8102598 ## 2 NA 10.8435907 ## 3 NA 18.6941528 ## 4 NA 12.3253975 ## 5 NA 8.1776628 ## 6 NA 52.0793648 ## filterString ## 1 FTMS + p NSI Full ms [380.0000-1500.0000] ## 2 ITMS + c NSI t d Full ms2 652.3426@cid35.00 [174.0000-1968.0000] ## 3 ITMS + c NSI t d Full ms2 647.6781@cid35.00 [173.0000-1954.0000] ## 4 ITMS + c NSI t d Full ms2 673.6693@cid35.00 [180.0000-2000.0000] ## 5 ITMS + c NSI t d Full ms2 575.0026@cid35.00 [153.0000-1736.0000] ## 6 FTMS + p NSI sps d Full ms3 673.6693@cid35.00 490.2629@hcd65.00 [100.0000-500.0000] ## spectrumId centroided ## 1 controllerType=0 controllerNumber=1 scan=32918 FALSE ## 2 controllerType=0 controllerNumber=1 scan=32919 TRUE ## 3 controllerType=0 controllerNumber=1 scan=32920 TRUE ## 4 controllerType=0 controllerNumber=1 scan=32921 TRUE ## 5 controllerType=0 controllerNumber=1 scan=32922 TRUE ## 6 controllerType=0 controllerNumber=1 scan=32923 FALSE ## ionMobilityDriftTime isolationWindowTargetMZ isolationWindowLowerOffset ## 1 NA NA NA ## 2 NA 652.34 0.35 ## 3 NA 647.68 0.35 ## 4 NA 673.67 0.35 ## 5 NA 575.00 0.35 ## 6 NA 490.26 NA ## isolationWindowUpperOffset scanWindowLowerLimit scanWindowUpperLimit ## 1 NA 380 1500 ## 2 0.35 174 1968 ## 3 0.35 173 1954 ## 4 0.35 180 2000 ## 5 0.35 153 1736 ## 6 NA 100 500 # Check annotation of specific spectra hdr_sp &lt;- header(msfile, 8) head(hdr_sp) ## seqNum acquisitionNum msLevel polarity peaksCount totIonCurrent retentionTime ## 1 8 32925 2 1 858 627600.3 4423.094 ## basePeakMZ basePeakIntensity collisionEnergy ionisationEnergy lowMZ ## 1 663.859 28716.19 35 0 182.791 ## highMZ precursorScanNum precursorMZ precursorCharge precursorIntensity ## 1 1715.989 32918 675.6938 3 4257284 ## mergedScan mergedResultScanNum mergedResultStartScanNum ## 1 NA NA NA ## mergedResultEndScanNum injectionTime ## 1 NA 16.85191 ## filterString ## 1 ITMS + c NSI t d Full ms2 675.6938@cid35.00 [181.0000-2000.0000] ## spectrumId centroided ## 1 controllerType=0 controllerNumber=1 scan=32925 TRUE ## ionMobilityDriftTime isolationWindowTargetMZ isolationWindowLowerOffset ## 1 NA 675.69 0.35 ## isolationWindowUpperOffset scanWindowLowerLimit scanWindowUpperLimit ## 1 0.35 181 2000 After obtaining the annotation, I want to visualize the data I have up until this moment. This can be done with the MSnbase package and the msdata package. The readMSdata function of the MSnbase package takes as input the file path to one or multiple raw mass spectrometry files. It generates an MSnExp object. # Load library library(MSnbase) # Merge annotation with spectra mse &lt;- readMSData(path, mode = &quot;onDisk&quot;, verbose = FALSE) mse ## MSn experiment data (&quot;OnDiskMSnExp&quot;) ## Object size in memory: 0.32 Mb ## - - - Spectra data - - - ## MS level(s): 1 2 3 ## Number of spectra: 565 ## MSn retention times: 73:43 - 74:54 minutes ## - - - Processing information - - - ## Data loaded [Wed Aug 23 12:30:24 2023] ## MSnbase version: 2.24.2 ## - - - Meta data - - - ## phenoData ## rowNames: MS3TMT10_01022016_32917-33481.mzML.gz ## varLabels: sampleNames ## varMetadata: labelDescription ## Loaded from: ## MS3TMT10_01022016_32917-33481.mzML.gz ## protocolData: none ## featureData ## featureNames: F1.S001 F1.S002 ... F1.S565 (565 total) ## fvarLabels: fileIdx spIdx ... spectrum (35 total) ## fvarMetadata: labelDescription ## experimentData: use &#39;experimentData(object)&#39; # Access individual spectra information mse[[8]] ## Object of class &quot;Spectrum2&quot; ## Precursor: 675.6938 ## Retention time: 73:43 ## Charge: 3 ## MSn level: 2 ## Peaks count: 858 ## Total ion count: 627600.3 # Check annotation and raw spectra data ## The output will not be shown to keep the page tidy. fData(mse) 6.6 Visualization of the data To visualize the spectral data with the annotations I created a function called visualizeSpectraWithAnnotations. This function takes the created mse object and an optional spectrumIndex parameter to specify which spectrum you want to visualize. Inside the function, I use mse[spectrumIndex] to access the spectral data for the selected spectrum. Next the attr function is used to access the annotation data associated with the spectrum. The spectra will then be plotted with the plot function. If annotation data is available, it will be printed to the console. You can customize this part to visualize or process the annotation data as needed. # Function to visualize spectral data with annotations visualizeSpectraWithAnnotations &lt;- function(mse, spectrumIndex = 1) { # Get the spectral data for the specified spectrumIndex spectrum &lt;- mse[spectrumIndex] # Get the annotation data for the specified spectrumIndex annotation_data &lt;- attr(spectrum, &quot;SpectraAnnotations&quot;) # Plot the spectrum plot(spectrum) # Check if annotation data is available for the selected spectrum if (!is.null(annotation_data)) { # Print the annotation data print(annotation_data) } } # Usage: ## Assuming the &#39;mse&#39; object is already created ## Visualize the first spectrum with annotations visualizeSpectraWithAnnotations(mse, spectrumIndex = 1) 6.7 Analyzing MALDI-TOF MS data Now that I have visualized raw mass spectrometry data, it is time for the real challenge. In the beginning of this chapter I already mentioned that I wanted to visualize MALDI-TOF MS data. For this instance I need to look for a package that works with this format. After some research I found the MALDIquant package. This package is specifically designed for MALDI-TOF MS data. To demonstrate the visualization of MALDI-TOS MS data I used the fiedler2009subset dataset. This data is a list of 16 mass spectrum objects and is available with the MALDIquant package. # Load library library(MALDIquant) # Load data data(&quot;fiedler2009subset&quot;) # Quick inspection of two of the spectra plot(fiedler2009subset[[2]]) plot(fiedler2009subset[[8]]) The identification of specific proteins or bacteria can be done in different ways. For example, a specific m/z value can be highlighted. It can also be useful to be able to highlight a known peak pattern, in this case of a bacterium. First of all, I tried to highlight a specific m/z value or at least indicate it in an entire spectra. To achieve this I wrote a function which takes as input the file with the m/z values, the desired m/z value to be highlighted and the tolerance. For example, if you set the mz_value parameter to 300.0 and the tolerance parameter to 0.1, the function will search for a peak in the spectra with an m/z value that is within ±0.1 of 300.0. If a peak is found within this range, it will be considered a match and highlighted in the plot. # Load library library(ggplot2) # Create function highlight_mz &lt;- function(mzml_path, mz_value, tolerance = 0.1) { # Open mzML file msfile &lt;- openMSfile(path, backend = NULL, verbose = FALSE) # Find the nearest peak to the desired m/z value nearest_peak &lt;- NULL nearest_distance &lt;- Inf spectra &lt;- spectra(msfile, 1) for (i in 1:length(spectra)) { peaks &lt;- peaks(msfile, 8) distances &lt;- abs(peaks[, 1] - mz_value) min_distance &lt;- min(distances) if (min_distance &lt; nearest_distance) { nearest_peak &lt;- peaks[which(distances == min_distance), ] nearest_distance &lt;- min_distance } } # Highlight the nearest peak and create a plot if (!is.null(nearest_peak) &amp;&amp; nearest_distance &lt;= tolerance) { nearest_peak$intensity &lt;- 1.0 # Set intensity to 1.0 to highlight # Convert nearest_peak to data frame nearest_peak &lt;- as.data.frame(nearest_peak) # Plot the spectrum p &lt;- ggplot(data = nearest_peak, aes(x = mz, y = intensity)) + geom_line() + geom_point(color = &quot;red&quot;, size = 2) + labs(x = &quot;m/z value&quot;, y = &quot;Intensity&quot;, title = &quot;Highlighted m/z Spectrum&quot;) print(p) } else { cat(&quot;No peak found within the specified tolerance.\\n&quot;) } } # Example usage highlight_mz(&quot;msfile&quot;, 700.0, tolerance = 1.0) As you can see the code is not working optimally. The indicated m/z value is indicated in the plot, but the spectra are not present. This is due to the fact that the fiedler2009subset is a list. For this reason I will try a different approach. In the code below I created a loop that goes over the 16 spectra. First you can set the highlight_mz variable to the specific m/z value you want to highlight in the plots, in this case 3000. The next line opens a PDF file for saving the individual mass spectrometry plots named maldi_plot.pdf. The loop iterates through each MassSpectrum object in the fiedler2009subset dataset. Then the m/z and intensity values are extracted from the current MassSpectrum object using as.matrix. Finally, a plot is made of the mass spectra with the m/z values and the intensity values. The previously specified m/z value will be highlighted and all the plots will be visible in the maldi_plot.pdf file. This file is available in the data folder of this page. To show the output of this code two of the plots are displayed in this Rmarkdown. # Select the specific m/z value you want to highlight highlight_mz &lt;- 3000 # Create a plot for each MassSpectrum in the list pdf(&quot;maldi_plot.pdf&quot;) # Change the filename and format if needed # Loop through the list of MassSpectrum objects for (i in 1:length(fiedler2009subset)) { ms &lt;- fiedler2009subset[[i]] # Extract the m/z and intensity values using as.matrix ms_matrix &lt;- as.matrix(ms) mz_values &lt;- ms_matrix[, 1] intensity_values &lt;- ms_matrix[, 2] # Plot the mass spectrum plot(mz_values, intensity_values, type = &quot;l&quot;, col = &quot;blue&quot;, xlim = c(1000, 10000), ylim = c(0, max(intensity_values) + 100), xlab = &quot;m/z&quot;, ylab = &quot;Intensity&quot;) # Add a vertical line to highlight the specific m/z value abline(v = highlight_mz, col = &quot;red&quot;) # Add a title to the plot title(paste(&quot;Spectrum&quot;, i)) # Pause briefly to view each plot (optional) Sys.sleep(2) } # Close the PDF file dev.off() Spectrum 1 of the fiedler2009subset dataset Spectrum 2 of the fiedler2009subset dataset 6.8 Conclusion Looking back at the initial planning I can say that it is possible to visualize MALDI-TOF MS data in R and make a specific m/z value visible in a plot. With the written code, a clear pdf-file is now made of multiple spectra within a dataset. Also a specific m/z value can be highlighted within the spectra. In the future I would like to extend this code, so that the spectra of a dataset can be compared with the spectra of a specific bacterium. "],["introduction-of-the-antimicrobial-resistance-project.html", "Chapter 7 Introduction of the antimicrobial resistance project 7.1 Antimicrobial resistance 7.2 Bacterial conjugation 7.3 Illumina paired-end sequencing", " Chapter 7 Introduction of the antimicrobial resistance project At the moment I am following the course Data Science for Biology 2 at the Hogeschool Utrecht. In addition of this course we need to fulfill a project in groups of 2 or 3 students. My group got the subject Antimicrobial resistance (AMR). The aim of this project is to map the distribution of AMR genes (which are present on bacterial plasmids) in the Netherlands. This using illumina paired-end sequencing data. This data comes from the SRA database: https://www.ncbi.nlm.nih.gov/sra To gather more information about this subject it is required to write an introduction for our project. In the text below I wrote a introduction for our project, with the main focus on the following questions: - What is AMR? - How do bacteria spread their resistance genes? - What is conjugation? - What is illumina sequencing? 7.1 Antimicrobial resistance Antimicrobial resistance is the ability of micro-organisms to resist antimicrobial treatments. Bacteria wich are resistant to at least three different classes of antimicrobials, defined as multidrug resistant (MDR), have become commonplace. This is especially the case in hospitals. Once a single bacterium mutates to become resistant to antibiotics, it can transfer that resistance to other bacteria around it through a process known as horizontal gene transfer. One of the main vehicles for gene transfer among bacteria are small circular pieces of DNA, or plasmids. Plasmids can be transferred through direct physical contact between bacteria in a process known as conjugation, which helps bacteria share their antibiotic resistance genes with their neighbors. (“Plasmids and the Spread of Antibiotic Resistance Genes” n.d.) 7.2 Bacterial conjugation Bacterial conjugation is one of the three major known modes of genetic exchange between bacteria, the other two being transduction and bacterial transformation.(Raleigh and Low 2013) Conjugation is the transfer of a plasmid or other self-transmissible DNA element and sometimes chromosomal DNA from a donor cell to a recipient cell via direct contact usually mediated by a conjugation pilus or sex pilus. (Llosa et al. 2002) 7.3 Illumina paired-end sequencing For this project illumina paired-end sequencing data was gathered to map the information about the distribution of AMR genes in the Netherlands. The workflow of illumina next generation sequencing can be divided into four steps. This includes library preparation, cluster generation, sequencing and alignment/ data-analysis. (“Next-Generation Sequencing for Beginners | NGS Basics for Researchers” n.d.) First the DNA is fragmented using ultrasonic fragmentation. This produces fragments of 200-500 bp in lenght. The 5’ and 3’ adapter are added to the two ends of these small segments. The DNA fragments in the sequencing library will randomly attach to the lanes on the surface of the flow cell when they pass through it. Flow cell is a channel for adsorbing mobile DNA fragments, and it’s also a core sequencing reactor vessel. Each flow cell has 8 Lanes, each lane has a number of adapters attached to the surface, which can match the adapters added at the ends of the DNA fragment in the building process. Bridge PCR was performed using the adapters on flow cell surface as template. After continuous amplification and mutation cycles, each DNA fragment will eventually be clustered in bundles at their respective locations. The sequencing method is based on sequencing-by-synthesis (SBS). DNA polymerase, connector primers and 4 dNTP with base-specific fluorescent markers are added to the reaction system. The 3′-OH of these dNTP are protected by chemical methods, which ensures that only one base will be added at a time during the sequencing process. All unused free dNTP and DNA polymerase are eluted after the synthesis reaction finished. Then, buffer solution needed for fluorescence excitation are added, the fluorescence signal is excited by laser, and fluorescence signal is recorded by optical equipment. Finally, the optical signal is converted into sequencing base by computer analysis. When the fluorescence signal is recorded, a chemical reagent is added to quench the fluorescence signal and remove the dNTP 3′-OH protective group, so that the next round of sequencing reaction can be performed. The newly identified sequence reads can be aligned to a reference genome. (cdadmin 2018) References "],["relational-databases.html", "Chapter 8 Relational databases 8.1 Introduction 8.2 Required packages 8.3 Tidy data 8.4 Inspecting the data with DBeaver 8.5 Inspecting the data with dplyr 8.6 Data visualisation 8.7 Conclusion", " Chapter 8 Relational databases 8.1 Introduction On this page data about flu and dengue fever occurrences between 2002 and 2015 will be analyzed. This data is from Google Dengue Trends weekly dengue activity for the world and Google Flu Trends weekly influenza activity estimates for the world. The used data can aslo be downloaded here: Data Source: Google Dengue Trends (http://www.google.org/denguetrends) Data Source: Google Flu Trends (http://www.google.org/flutrends) https://github.com/DataScienceILC/tlsc-dsfb26v-20_workflows/tree/main/data 8.2 Required packages The following packages must be installed to perform the code. I highly recommend to download the required packages before running the code. # install.packages(&quot;gapminder&quot;) # install.packages(&quot;tidyverse&quot;) # install.packages(&quot;RPostgreSQL&quot;) # install.packages(&quot;Rpostgres&quot;) # install.packages(&quot;devtools&quot;) # install.packages(&quot;remotes&quot;) # install.packages(&quot;dplyr&quot;) # install.packages(&quot;ggplot2&quot;) # install.packages(&quot;dslabs&quot;) # install.packages(&quot;here&quot;) 8.3 Tidy data Before we can analyze the data, the data must be tidied. The following code was used to load and tidy the datasets. # Load library library(tidyverse) # Load datasets into dataframes # Change the file path to your own structure data_flu &lt;- read_csv((&quot;./Data_raw/flu_data.csv&quot;), skip = 11) %&gt;% as_tibble() data_dengue &lt;- read_csv((&quot;./Data_raw/dengue_data.csv&quot;), skip = 11) %&gt;% as_tibble() data_gapminder &lt;- dslabs::gapminder %&gt;% as_tibble() # Tidy the data tidy_flu &lt;- data_flu %&gt;% pivot_longer(cols = Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;searches&quot;) tidy_dengue &lt;- data_dengue %&gt;% pivot_longer(cols = Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;searches&quot;) # Check the datasets str(data_flu) ## tibble [659 × 30] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:659], format: &quot;2002-12-29&quot; &quot;2003-01-05&quot; ... ## $ Argentina : num [1:659] NA NA NA NA NA 136 145 141 135 134 ... ## $ Australia : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Austria : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Belgium : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Bolivia : num [1:659] NA NA NA NA NA NA NA NA 426 427 ... ## $ Brazil : num [1:659] 174 162 174 162 131 151 184 162 194 177 ... ## $ Bulgaria : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Canada : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Chile : num [1:659] NA NA 1 0 0 0 0 0 0 0 ... ## $ France : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Germany : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Hungary : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Japan : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Mexico : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Netherlands : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ New Zealand : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Norway : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Paraguay : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Peru : num [1:659] 329 315 314 267 241 227 250 236 274 270 ... ## $ Poland : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Romania : num [1:659] NA NA NA NA NA NA 664 736 740 864 ... ## $ Russia : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ South Africa : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Spain : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Sweden : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Switzerland : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Ukraine : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ United States: num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Uruguay : num [1:659] NA NA NA NA NA NA NA NA NA NA ... str(data_dengue) ## tibble [659 × 11] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:659], format: &quot;2002-12-29&quot; &quot;2003-01-05&quot; ... ## $ Argentina : num [1:659] NA NA NA NA NA NA NA 0.046 0.048 0.051 ... ## $ Bolivia : num [1:659] 0.101 0.143 0.176 0.173 0.146 0.16 0.225 0.109 0.147 0.119 ... ## $ Brazil : num [1:659] 0.073 0.098 0.119 0.17 0.138 0.202 0.179 0.239 0.205 0.142 ... ## $ India : num [1:659] 0.062 0.047 0.051 0.032 0.04 0.038 0.019 0.008 0.022 0.028 ... ## $ Indonesia : num [1:659] 0.101 0.039 0.059 0.039 0.112 0.049 0.06 0.039 0.078 0.048 ... ## $ Mexico : num [1:659] NA NA 0.071 0.052 0.048 0.041 0.042 0.049 0.054 0.075 ... ## $ Philippines: num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Singapore : num [1:659] 0.059 0.059 0.238 0.175 0.164 0.163 0.15 0.144 0.142 0.129 ... ## $ Thailand : num [1:659] NA NA NA NA NA NA NA NA NA NA ... ## $ Venezuela : num [1:659] NA NA NA NA NA NA NA 0.139 0.137 0.168 ... str(data_gapminder) ## tibble [10,545 × 9] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 185 levels &quot;Albania&quot;,&quot;Algeria&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ year : int [1:10545] 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... ## $ infant_mortality: num [1:10545] 115.4 148.2 208 NA 59.9 ... ## $ life_expectancy : num [1:10545] 62.9 47.5 36 63 65.4 ... ## $ fertility : num [1:10545] 6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ... ## $ population : num [1:10545] 1636054 11124892 5270844 54681 20619075 ... ## $ gdp : num [1:10545] NA 1.38e+10 NA NA 1.08e+11 ... ## $ continent : Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 4 1 1 2 2 3 2 5 4 3 ... ## $ region : Factor w/ 22 levels &quot;Australia and New Zealand&quot;,..: 19 11 10 2 15 21 2 1 22 21 ... After changing the data to a tidy format, the date column must be differentiated into three different columns. The new columns will be named day, month and year. # Split the Date column to three different columns named day, month and year tidy_flu &lt;- tidy_flu %&gt;% separate(Date, c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) tidy_dengue &lt;- tidy_dengue %&gt;% separate(Date, c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) # Changing the data to the correct data class ## Flu data tidy_flu$year &lt;- as.numeric(tidy_flu$year) tidy_flu$month &lt;- as.numeric(tidy_flu$month) tidy_flu$day &lt;- as.numeric(tidy_flu$day) ## Dengue data tidy_dengue$year &lt;- as.numeric(tidy_dengue$year) tidy_dengue$month &lt;- as.numeric(tidy_dengue$month) tidy_dengue$day &lt;- as.numeric(tidy_dengue$day) # Check the tidy data str(tidy_dengue) ## tibble [6,590 × 5] (S3: tbl_df/tbl/data.frame) ## $ year : num [1:6590] 2002 2002 2002 2002 2002 ... ## $ month : num [1:6590] 12 12 12 12 12 12 12 12 12 12 ... ## $ day : num [1:6590] 29 29 29 29 29 29 29 29 29 29 ... ## $ country : chr [1:6590] &quot;Argentina&quot; &quot;Bolivia&quot; &quot;Brazil&quot; &quot;India&quot; ... ## $ searches: num [1:6590] NA 0.101 0.073 0.062 0.101 NA NA 0.059 NA NA ... str(tidy_flu) ## tibble [19,111 × 5] (S3: tbl_df/tbl/data.frame) ## $ year : num [1:19111] 2002 2002 2002 2002 2002 ... ## $ month : num [1:19111] 12 12 12 12 12 12 12 12 12 12 ... ## $ day : num [1:19111] 29 29 29 29 29 29 29 29 29 29 ... ## $ country : chr [1:19111] &quot;Argentina&quot; &quot;Australia&quot; &quot;Austria&quot; &quot;Belgium&quot; ... ## $ searches: num [1:19111] NA NA NA NA NA 174 NA NA NA NA ... The tidy datasets will now be stored in new .csv and .rds files. The new files are stored in the data folder. # Tidy data to new .csv file tidy_flu %&gt;% write.csv(file = &quot;./Data/tidy_flu.csv&quot;) tidy_dengue %&gt;% write.csv(file = &quot;./Data/tidy_denque.csv&quot;) data_gapminder %&gt;% write.csv(file = &quot;./Data/gapminder.csv&quot;) # Tidy data to new .rds file saveRDS(tidy_flu, file = &quot;./Data/tidy_flu.rds&quot;) saveRDS(tidy_dengue, file = &quot;./Data/tidy_denque.rds&quot;) saveRDS(data_gapminder, file = &quot;./Data/gapminder.rds&quot;) 8.4 Inspecting the data with DBeaver After putting the datasets in seperate files, the data will be stored using DBeaver. To establish this, the 23.0.5 version of DBeaver was used. # Load library library(&quot;RPostgres&quot;) library(&quot;RPostgreSQL&quot;) library(&quot;remotes&quot;) library(&quot;devtools&quot;) # Export the tidy data to DBeaver ## To export the data to DBeaver requires a password, wich is unique for all users. The password in the code is a fake password for privacy reasons. # Open conncention con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;, user = &quot;postgres&quot;, password = &quot;datascience12&quot;) # Write to DBeaver dbWriteTable(con, &quot;tidy_flu&quot;, tidy_flu) dbWriteTable(con, &quot;tidy_dengue&quot;, tidy_dengue) dbWriteTable(con, &quot;data_gapminder&quot;, data_gapminder) # Close connection dbDisconnect(con) # Inspect the data in Dbeaver ## The following steps are the steps taken in DBeaver. For the impression of the code the tidy_dengue dataset was used. In DBeaver the code was performed for all 3 datasets. # SELECT &quot;Searches&quot;, &quot;Country&quot; # FROM tidy_dengue td order # BY &quot;Searches&quot; asc # SELECT &quot;Searches&quot;, &quot;Country&quot; # FROM tidy_dengue td order # BY &quot;Searches&quot; desc 8.5 Inspecting the data with dplyr We have inspected the datasets in DBeaver, but it is also possible to inspect the data with the dplyr package in R. # Load library library(dplyr) # Inspect the data glimpse(tidy_flu) ## Rows: 19,111 ## Columns: 5 ## $ year &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2… ## $ month &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1… ## $ day &lt;dbl&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2… ## $ country &lt;chr&gt; &quot;Argentina&quot;, &quot;Australia&quot;, &quot;Austria&quot;, &quot;Belgium&quot;, &quot;Bolivia&quot;, &quot;B… ## $ searches &lt;dbl&gt; NA, NA, NA, NA, NA, 174, NA, NA, NA, NA, NA, NA, NA, NA, NA, … glimpse(tidy_dengue) ## Rows: 6,590 ## Columns: 5 ## $ year &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2… ## $ month &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1, 1, 1, 1, 1, 1, 1, … ## $ day &lt;dbl&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 5, 5, 5, 5, 5, 5, 5, … ## $ country &lt;chr&gt; &quot;Argentina&quot;, &quot;Bolivia&quot;, &quot;Brazil&quot;, &quot;India&quot;, &quot;Indonesia&quot;, &quot;Mexi… ## $ searches &lt;dbl&gt; NA, 0.101, 0.073, 0.062, 0.101, NA, NA, 0.059, NA, NA, NA, 0.… glimpse(data_gapminder) ## Rows: 10,545 ## Columns: 9 ## $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Antigua and Barbuda&quot;… ## $ year &lt;int&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,… ## $ infant_mortality &lt;dbl&gt; 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.… ## $ life_expectancy &lt;dbl&gt; 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8… ## $ fertility &lt;dbl&gt; 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,… ## $ population &lt;dbl&gt; 1636054, 11124892, 5270844, 54681, 20619075, 1867396,… ## $ gdp &lt;dbl&gt; NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778… ## $ continent &lt;fct&gt; Europe, Africa, Africa, Americas, Americas, Asia, Ame… ## $ region &lt;fct&gt; Southern Europe, Northern Africa, Middle Africa, Cari… After we have inspected our data we can join the datasets. Before joining the data, we will first save the new gapminder dataframe to DBeaver. # Be aware of changing the dbname, host, password and user to your own settings when using this code. # Open conncention con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;, user = &quot;postgres&quot;, password = &quot;datascience12&quot;) # Write to DBeaver dbWriteTable(con, &quot;gapminder_clean&quot;, gapminder_data_clean, overwrite = TRUE) # Close connection dbDisconnect(con) Now that the new gapminder data is saved to DBeaver we can join the datasets. # Joining the flu and dengue data joined_data &lt;- full_join(tidy_flu, tidy_dengue, by = c(&quot;country&quot;, &quot;year&quot;), suffix = c(&quot;_flu&quot;, &quot;_dengue&quot;)) # Joining with gapminder joined_data &lt;- inner_join(joined_data, data_gapminder, by = c(&quot;country&quot;, &quot;year&quot;)) # Check str(joined_data) ## tibble [155,161 × 15] (S3: tbl_df/tbl/data.frame) ## $ year : num [1:155161] 2002 2002 2002 2002 2002 ... ## $ month_flu : num [1:155161] 12 12 12 12 12 12 12 12 12 12 ... ## $ day_flu : num [1:155161] 29 29 29 29 29 29 29 29 29 29 ... ## $ country : chr [1:155161] &quot;Argentina&quot; &quot;Australia&quot; &quot;Austria&quot; &quot;Belgium&quot; ... ## $ searches_flu : num [1:155161] NA NA NA NA NA 174 NA NA NA NA ... ## $ month_dengue : num [1:155161] 12 NA NA NA 12 12 NA NA NA NA ... ## $ day_dengue : num [1:155161] 29 NA NA NA 29 29 NA NA NA NA ... ## $ searches_dengue : num [1:155161] NA NA NA NA 0.101 0.073 NA NA NA NA ... ## $ infant_mortality: num [1:155161] 17.1 5 4.4 4.4 53.7 24.3 16.3 5.3 8.3 4.2 ... ## $ life_expectancy : num [1:155161] 74.3 80.3 78.8 78.2 68.7 71.4 72.1 79.6 77.7 79.4 ... ## $ fertility : num [1:155161] 2.38 1.76 1.39 1.68 3.98 2.26 1.22 1.51 2.01 1.87 ... ## $ population : num [1:155161] 37889443 19514385 8114698 10364613 8653343 ... ## $ gdp : num [1:155161] 2.42e+11 4.42e+11 1.97e+11 2.38e+11 8.75e+09 ... ## $ continent : Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 2 5 4 4 2 2 4 2 2 4 ... ## $ region : Factor w/ 22 levels &quot;Australia and New Zealand&quot;,..: 15 1 22 22 15 15 7 12 15 22 ... 8.6 Data visualisation Now that we have a table with all the data we can perform some descriptive statistics. For now we will focus on the flu and dengue fever occurrences for each continent. # Load library library(ggplot2) # Flu occurences per continent joined_data %&gt;% group_by(year, continent) %&gt;% mutate(occurrences_flu = mean(searches_flu, na.rm = TRUE)) %&gt;% ggplot(aes(x = year, y = occurrences_flu)) + geom_line(aes(colour = continent))+ labs( title = &quot;Flu occurrences for each continent&quot;) # Dengue occurences per continent joined_data %&gt;% group_by(year, continent) %&gt;% mutate(occurrences_dengue = mean(searches_dengue, na.rm = TRUE)) %&gt;% ggplot(aes(x = year, y = occurrences_dengue)) + geom_line(aes(colour = continent)) + labs( title = &quot;Dengue occurrences for each continent&quot;) Based on the graphs above, the following conclusions can be drawn. The highest occurrence of flu is in Africa. The other continents have similar values. For dengue fever the occurrences in America and Asia are similar. The other continents have no occurrences of dengue fever between 2002 and 2015. To visualize the statistical difference between the occurrence of dengue fever and flu, the continent America was chosen to give an example of this test. # Subset data for America # You can change this part of the code to any continent you like america_flu &lt;- joined_data %&gt;% filter(continent == &quot;Americas&quot;) america_dengue &lt;- joined_data %&gt;% filter(continent == &quot;Americas&quot;) # Perform t-test t_test &lt;- t.test(america_flu$searches_flu, america_dengue$searches_dengue) # Display t-test results t_test ## ## Welch Two Sample t-test ## ## data: america_flu$searches_flu and america_dengue$searches_dengue ## t = 343.61, df = 135918, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 443.4828 448.5711 ## sample estimates: ## mean of x mean of y ## 446.1279954 0.1010518 Now that we have an overview of the dengue fever and flu occurrences for each continent, we can focus on the occurrences for each specific country. # Dengue cases per country joined_data %&gt;% ggplot() + geom_col(aes(x = country, y = searches_dengue, fill = country))+ labs(title = &quot;Dengue cases for each country&quot;, y=&quot;Dengue occurrences&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot;) # Flu cases per country joined_data %&gt;% ggplot() + geom_col(aes(x = country, y = searches_flu, fill = country))+ labs(title = &quot;Flu cases for each country&quot;, y=&quot;Flu occurrences&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot;) 8.7 Conclusion Based on the graphs above the following conclusions can be drawn. When considering the dengue occurrences per country, we can see that the highest occurrence is in Mexico, followed by Brazil, Argentina and Bolivia. When looking at the flu occurrences per country, we can see that the highest occurrence is in Mexico, followed by Bolivia, Brazil and Argentina. "],["parameterized-report-for-the-covid-19-cases.html", "Chapter 9 Parameterized report for the COVID-19 cases 9.1 Introduction 9.2 Required packages 9.3 Importing and filtering the data 9.4 Data visualisation", " Chapter 9 Parameterized report for the COVID-19 cases 9.1 Introduction In this report I will be analyzing the number of COVID-19 cases and deaths from 2020 till 2022. In this analysis I will be using parameters to visualize the number of COVID-19 cases and deaths. To execute this report I downloaded a dataset from the following website: https://www.ecdc.europa.eu/en/publications-data/data-daily-new-cases-covid-19-eueea-country 9.2 Required packages The following packages must be installed to perform the code. If you want to use this code I highly recommend to download the required packages before running the code. # install.packages(&quot;tidyverse&quot;) # install.packages(&quot;ggplot2&quot;) # install.packages(&quot;dplyr&quot;) 9.3 Importing and filtering the data For this report the data from Austria will be filtered to visualize the cases and deaths in 2022. # Load required libraries library(dplyr) # Import data from raw data folder covid_data &lt;- read.csv(file = &quot;./Data_raw/data.csv&quot;) # Set parameters # You can change these parameters to any you like country &lt;- &quot;Austria&quot; year &lt;- 2022 period &lt;- 10 # Filter the data based on parameters filtered_data &lt;- covid_data %&gt;% filter(countriesAndTerritories == country, year == year, month == period) 9.4 Data visualisation # Load library library(ggplot2) # Generate the graph for COVID-19 cases cases_plot &lt;- ggplot(filtered_data, aes(x = as.Date(paste(year, month, day, sep = &quot;-&quot;)), y = cases)) + geom_line() + labs(x = &quot;Date&quot;, y = &quot;Number of Cases&quot;, title = &quot;Number of COVID-19 cases&quot;) + theme_minimal() # See plot cases cases_plot # Generate the graph for COVID-19 deaths deaths_plot &lt;- ggplot(filtered_data, aes(x = as.Date(paste(year, month, day, sep = &quot;-&quot;)), y = deaths)) + geom_line() + labs(x = &quot;Date&quot;, y = &quot;Number of Deaths&quot;, title = &quot;Number of deaths caused by COVID-19 infection&quot;) + theme_minimal() # See plot deaths deaths_plot "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
